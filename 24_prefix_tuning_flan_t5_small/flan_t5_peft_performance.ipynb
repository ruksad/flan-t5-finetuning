{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "52444341",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!pip -q install -U \"transformers>=4.44\" \"datasets>=2.20\" \"peft>=0.12\" accelerate evaluate bert-score rouge-score scikit-learn sentencepiece \"pyarrow<20.0.0a0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "5b5d44b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and config\n",
    "import os\n",
    "import re\n",
    "from tqdm.auto import tqdm\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "from datasets import load_dataset\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f45b5492",
   "metadata": {},
   "source": [
    "## Load model and tokenizer\n",
    "We'll use the small FLAN-T5 model to keep things light.\n",
    "- Tokenizer converts text ↔ tokens\n",
    "- Model generates outputs given the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "bfb451b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "MODEL_NAME = \"google/flan-t5-small\"\n",
    "DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(\"device:\", DEVICE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "df2699c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer... This may take a minute\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32128, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 6)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-7): 7 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32128, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 6)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-7): 7 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=384, bias=False)\n",
       "              (o): Linear(in_features=384, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseGatedActDense(\n",
       "              (wi_0): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wi_1): Linear(in_features=512, out_features=1024, bias=False)\n",
       "              (wo): Linear(in_features=1024, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): NewGELUActivation()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32128, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Loading model and tokenizer... This may take a minute\")\n",
    "from transformers import logging as hf_logging\n",
    "hf_logging.set_verbosity_error()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME).to(DEVICE)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "297a7685",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T5Config {\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 1024,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"gelu_new\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"dtype\": \"float32\",\n",
      "  \"eos_token_id\": 1,\n",
      "  \"feed_forward_proj\": \"gated-gelu\",\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": true,\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 8,\n",
      "  \"num_heads\": 6,\n",
      "  \"num_layers\": 8,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"transformers_version\": \"4.57.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32128\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(model.config )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d7a6667d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hidden size (d_model): 512\n",
      "Encoder layers: 8\n",
      "Decoder layers: 8\n",
      "Number of attention heads: 6\n",
      "Key-value dimension per head: 64\n",
      "Total Q/K/V dimension: 384\n"
     ]
    }
   ],
   "source": [
    "print(f\"Hidden size (d_model): {model.config.d_model}\")  \n",
    "print(f\"Encoder layers: {model.config.num_layers}\")      \n",
    "print(f\"Decoder layers: {model.config.num_decoder_layers}\")  \n",
    "\n",
    "print(f\"Number of attention heads: {model.config.num_heads}\")\n",
    "print(f\"Key-value dimension per head: {model.config.d_kv}\")\n",
    "print(f\"Total Q/K/V dimension: {model.config.num_heads * model.config.d_kv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b1a78e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "encoder.block.0.layer.0.SelfAttention.q.weight: torch.Size([384, 512])\n",
      "encoder.block.1.layer.0.SelfAttention.q.weight: torch.Size([384, 512])\n",
      "encoder.block.2.layer.0.SelfAttention.q.weight: torch.Size([384, 512])\n",
      "encoder.block.3.layer.0.SelfAttention.q.weight: torch.Size([384, 512])\n",
      "encoder.block.4.layer.0.SelfAttention.q.weight: torch.Size([384, 512])\n",
      "encoder.block.5.layer.0.SelfAttention.q.weight: torch.Size([384, 512])\n",
      "encoder.block.6.layer.0.SelfAttention.q.weight: torch.Size([384, 512])\n",
      "encoder.block.7.layer.0.SelfAttention.q.weight: torch.Size([384, 512])\n",
      "decoder.block.0.layer.0.SelfAttention.q.weight: torch.Size([384, 512])\n",
      "decoder.block.1.layer.0.SelfAttention.q.weight: torch.Size([384, 512])\n",
      "decoder.block.2.layer.0.SelfAttention.q.weight: torch.Size([384, 512])\n",
      "decoder.block.3.layer.0.SelfAttention.q.weight: torch.Size([384, 512])\n",
      "decoder.block.4.layer.0.SelfAttention.q.weight: torch.Size([384, 512])\n",
      "decoder.block.5.layer.0.SelfAttention.q.weight: torch.Size([384, 512])\n",
      "decoder.block.6.layer.0.SelfAttention.q.weight: torch.Size([384, 512])\n",
      "decoder.block.7.layer.0.SelfAttention.q.weight: torch.Size([384, 512])\n"
     ]
    }
   ],
   "source": [
    "# See all parameter names\n",
    "for name, param in model.named_parameters():\n",
    "    if 'SelfAttention' in name and 'q' in name:\n",
    "        print(f\"{name}: {param.shape}\")\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "65b6c8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters: 76,961,152\n",
      "trainable parameters: 76,961,152\n"
     ]
    }
   ],
   "source": [
    "# Total parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"Total parameters: {total_params:,}\")  # 76,961,152\n",
    "\n",
    "# trainable parameters\n",
    "trainable = sum(p.numel() for p in model.parameters() \n",
    "                  if p.requires_grad)\n",
    "\n",
    "print(f\"trainable parameters: {trainable:,}\")  # ~6,144,512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5e847aa6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query weight shape: torch.Size([384, 512])\n",
      "Key weight shape: torch.Size([384, 512])\n",
      "Value weight shape: torch.Size([384, 512])\n",
      "Output weight shape: torch.Size([512, 384])\n"
     ]
    }
   ],
   "source": [
    "# Check a specific attention layer\n",
    "encoder_attn = model.encoder.block[0].layer[0].SelfAttention\n",
    "\n",
    "print(\"Query weight shape:\", encoder_attn.q.weight.shape)  # (384, 512)\n",
    "print(\"Key weight shape:\", encoder_attn.k.weight.shape)    # (384, 512)\n",
    "print(\"Value weight shape:\", encoder_attn.v.weight.shape)  # (384, 512)\n",
    "print(\"Output weight shape:\", encoder_attn.o.weight.shape) # (384, 512)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2fd642",
   "metadata": {},
   "source": [
    "- Loads SST-2 and SAMSum from Hugging Face datasets.\n",
    "- Runs zero-shot classification on SST-2 using google/flan-t5-small (prompting the model to return exactly one label).\n",
    "- Runs zero-shot summarization on SAMSum (prompting the model for 1–2 sentence summaries).\n",
    "- Evaluates classification (accuracy) and summarization (ROUGE).\n",
    "- Uses small subsets by default so that we can iterate quickly on CPU/GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8d7e1caf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "max_examples = 200\n",
    "# Generation settings\n",
    "GEN_KWARGS_CLASS = {\n",
    "    \"max_length\": 16,\n",
    "    \"num_beams\": 5,\n",
    "    \"early_stopping\": True,\n",
    "    \"do_sample\": False,\n",
    "    \"temperature\": 0.0,\n",
    "}\n",
    "\n",
    "GEN_KWARGS_SUM = {\n",
    "    \"max_length\": 120,\n",
    "    \"num_beams\": 4,\n",
    "    \"early_stopping\": True,\n",
    "    \"do_sample\": False,\n",
    "    \"temperature\": 0.0,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0a374731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility: normalize model-generated text\n",
    "import unicodedata\n",
    "\n",
    "def normalize_text(s: str):\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    s = s.strip().lower()\n",
    "    # normalize unicode\n",
    "    s = unicodedata.normalize(\"NFKD\", s)\n",
    "    # remove punctuation except spaces\n",
    "    s = re.sub(r\"[^\\w\\s]\", \"\", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ebf84a",
   "metadata": {},
   "source": [
    "## Zero-shot classification (SST-2 style)\n",
    "FLAN-T5 understands instructions. For SST-2, prompting with `sst2: <text>` often produces `positive` or `negative`.\n",
    "We'll write a tiny helper to classify one or more texts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cc835a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SST-2 zero-shot: 100%|██████████| 200/200 [00:13<00:00, 14.78it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SST-2 zero-shot accuracy on 200 examples: 0.8600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "def zero_shot_sst2_classify(ds,labels=[\"positive\", \"negative\"]):    \n",
    "\n",
    "    preds = []\n",
    "    sentence = []\n",
    "    true_labels = [\"negative\" if sentence[\"label\"] == 0 else \"positive\" for sentence in ds]\n",
    "\n",
    "    for ex in tqdm(ds, desc=\"SST-2 zero-shot\"):\n",
    "        text = ex[\"sentence\"]\n",
    "        prompt = (\n",
    "            \"Classify the sentiment of the text as one of the following labels: \"\n",
    "            + \", \".join(labels)\n",
    "            + \".\\n\\n\"\n",
    "            + f\"Text: \\\"{text}\\\"\\n\\nAnswer with exactly one word: \"\n",
    "        )\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512).to(DEVICE)\n",
    "        out = model.generate(**inputs, **GEN_KWARGS_CLASS)\n",
    "        out_text = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "        out_text_norm = normalize_text(out_text)\n",
    "\n",
    "        mapped = None\n",
    "        for lab in labels:\n",
    "            if normalize_text(lab) == out_text_norm:\n",
    "                mapped = lab\n",
    "                break\n",
    "        if mapped is None:\n",
    "            for lab in labels:\n",
    "                if normalize_text(lab) in out_text_norm or out_text_norm in normalize_text(lab):\n",
    "                    mapped = lab\n",
    "                    break\n",
    "        if mapped is None:\n",
    "            for lab in labels:\n",
    "                if normalize_text(lab).split()[0] in out_text_norm:\n",
    "                    mapped = lab\n",
    "                    break\n",
    "        if mapped is None:\n",
    "            mapped = labels[0]\n",
    "            print(\"Warning: couldn't map output:\", out_text, \"-> falling back to\", mapped)\n",
    "\n",
    "        preds.append(mapped)\n",
    "        sentence.append(text)\n",
    "\n",
    "    # compute accuracy\n",
    "    acc = sum(1 for p, t in zip(preds, true_labels) if p == t) / len(preds)\n",
    "    print(f\"SST-2 zero-shot accuracy on {len(preds)} examples: {acc:.4f}\")\n",
    "    return {\"sentence\": sentence, \"preds\": preds, \"trues\": true_labels, \"accuracy\": acc}\n",
    "\n",
    "ds = load_dataset(\"glue\", \"sst2\", split=\"validation\")\n",
    "if max_examples:\n",
    "    ds = ds.select(range(min(len(ds), max_examples)))\n",
    "\n",
    "# Run classification (adjust MAX_EXAMPLES if needed)\n",
    "sst2_res = zero_shot_sst2_classify(ds, labels=[\"positive\", \"negative\"])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efbf9f13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 sentence:  it 's a charming and often affecting journey .  pred: positive true: positive\n",
      "1 sentence:  unflinchingly bleak and desperate  pred: negative true: negative\n",
      "2 sentence:  allows us to hope that nolan is poised to embark a major career as a commercial yet inventive filmmaker .  pred: positive true: positive\n",
      "3 sentence:  the acting , costumes , music , cinematography and sound are all astounding given the production 's austere locales .  pred: positive true: positive\n",
      "4 sentence:  it 's slow -- very , very slow .  pred: negative true: negative\n",
      "5 sentence:  although laced with humor and a few fanciful touches , the film is a refreshingly serious look at young women .  pred: positive true: positive\n",
      "6 sentence:  a sometimes tedious film .  pred: negative true: negative\n",
      "7 sentence:  or doing last year 's taxes with your ex-wife .  pred: negative true: negative\n",
      "8 sentence:  you do n't have to know about music to appreciate the film 's easygoing blend of comedy and romance .  pred: positive true: positive\n",
      "9 sentence:  in exactly 89 minutes , most of which passed as slowly as if i 'd been sitting naked on an igloo , formula 51 sank from quirky to jerky to utter turkey .  pred: negative true: negative\n",
      "10 sentence:  the mesmerizing performances of the leads keep the film grounded and keep the audience riveted .  pred: positive true: positive\n",
      "11 sentence:  it takes a strange kind of laziness to waste the talents of robert forster , anne meara , eugene levy , and reginald veljohnson all in the same movie .  pred: negative true: negative\n",
      "12 sentence:  ... the film suffers from a lack of humor ( something needed to balance out the violence ) ...  pred: negative true: negative\n",
      "13 sentence:  we root for ( clara and paul ) , even like them , though perhaps it 's an emotion closer to pity .  pred: positive true: positive\n",
      "14 sentence:  even horror fans will most likely not find what they 're seeking with trouble every day ; the movie lacks both thrills and humor .  pred: negative true: negative\n",
      "15 sentence:  a gorgeous , high-spirited musical from india that exquisitely blends music , dance , song , and high drama .  pred: positive true: positive\n",
      "16 sentence:  the emotions are raw and will strike a nerve with anyone who 's ever had family trauma .  pred: negative true: positive\n",
      "17 sentence:  audrey tatou has a knack for picking roles that magnify her outrageous charm , and in this literate french comedy , she 's as morning-glory exuberant as she was in amélie .  pred: positive true: positive\n",
      "18 sentence:  ... the movie is just a plain old monster .  pred: negative true: negative\n",
      "19 sentence:  in its best moments , resembles a bad high school production of grease , without benefit of song .  pred: negative true: negative\n"
     ]
    }
   ],
   "source": [
    "# Show a few classification examples\n",
    "for i in range(20):\n",
    "    print(i, \"sentence: \", sst2_res[\"sentence\"][i], \"pred:\", sst2_res[\"preds\"][i], \"true:\", sst2_res[\"trues\"][i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236a2bae",
   "metadata": {},
   "source": [
    "## Classification evaluation (accuracy, precision/recall/f1, confusion matrix, CI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b93f4a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.8600\n",
      "Macro Precision: 0.8625, Macro Recall: 0.8596, Macro F1: 0.8596\n",
      "\n",
      "Per-class (label order = ['negative','positive']):\n",
      "  negative: precision=0.835, recall=0.901, f1=0.867, support=101\n",
      "  positive: precision=0.890, recall=0.818, f1=0.853, support=99\n",
      "\n",
      "\n",
      "Classification report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative     0.8349    0.9010    0.8667       101\n",
      "    positive     0.8901    0.8182    0.8526        99\n",
      "\n",
      "    accuracy                         0.8600       200\n",
      "   macro avg     0.8625    0.8596    0.8596       200\n",
      "weighted avg     0.8622    0.8600    0.8597       200\n",
      "\n",
      "Confusion matrix (rows=true, cols=pred):\n",
      "[[91 10]\n",
      " [18 81]]\n",
      "Accuracy 95% CI (bootstrap): [0.8100, 0.9050]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix, classification_report\n",
    "import numpy as np\n",
    "from math import sqrt\n",
    "import random\n",
    "\n",
    "# preds & trues should be lists from your notebook (sst2_res[\"preds\"], sst2_res[\"trues\"])\n",
    "preds = sst2_res[\"preds\"]\n",
    "trues = sst2_res[\"trues\"]\n",
    "\n",
    "# Basic metrics\n",
    "acc = accuracy_score(trues, preds)\n",
    "precision, recall, f1, support = precision_recall_fscore_support(trues, preds, labels=[\"negative\", \"positive\"], average=None)\n",
    "macro_precision, macro_recall, macro_f1, _ = precision_recall_fscore_support(trues, preds, average=\"macro\")\n",
    "\n",
    "print(\"Accuracy: {:.4f}\".format(acc))\n",
    "print(\"Macro Precision: {:.4f}, Macro Recall: {:.4f}, Macro F1: {:.4f}\".format(macro_precision, macro_recall, macro_f1))\n",
    "print(\"\\nPer-class (label order = ['negative','positive']):\")\n",
    "for lbl, p, r, f, s in zip([\"negative\",\"positive\"], precision, recall, f1, support):\n",
    "    print(f\"  {lbl}: precision={p:.3f}, recall={r:.3f}, f1={f:.3f}, support={s}\")\n",
    "\n",
    "print(\"\\n\\nClassification report:\")\n",
    "print(classification_report(trues, preds, digits=4))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(trues, preds, labels=[\"negative\", \"positive\"])\n",
    "print(\"Confusion matrix (rows=true, cols=pred):\")\n",
    "print(cm)\n",
    "\n",
    "# Bootstrapped 95% CI for accuracy\n",
    "def bootstrap_confidence_interval(preds, trues, metric_fn, n_boot=1000, alpha=0.05, seed=42):\n",
    "    rng = random.Random(seed)\n",
    "    n = len(preds)\n",
    "    stats = []\n",
    "    for _ in range(n_boot):\n",
    "        idxs = [rng.randrange(n) for _ in range(n)]\n",
    "        p_sample = [preds[i] for i in idxs]\n",
    "        t_sample = [trues[i] for i in idxs]\n",
    "        stats.append(metric_fn(t_sample, p_sample))\n",
    "    stats = np.array(stats)\n",
    "    lo = np.percentile(stats, 100 * (alpha / 2))\n",
    "    hi = np.percentile(stats, 100 * (1 - alpha / 2))\n",
    "    return lo, hi\n",
    "\n",
    "acc_lo, acc_hi = bootstrap_confidence_interval(preds, trues, lambda y_true, y_pred: accuracy_score(y_true, y_pred), n_boot=1000)\n",
    "print(f\"Accuracy 95% CI (bootstrap): [{acc_lo:.4f}, {acc_hi:.4f}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "597e1b85",
   "metadata": {},
   "source": [
    "## Zero-shot summarization\n",
    "For summarization, prefix the input with `summarize:` and provide the content (e.g., a short dialogue)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "fa0cdd38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SAMSum zero-shot: 100%|██████████| 200/200 [01:50<00:00,  1.82it/s]\n"
     ]
    }
   ],
   "source": [
    "# Cell: Zero-shot summarization on SAMSum\n",
    "\n",
    "def zero_shot_samsum_summarization(ds_samsum, summary_sentences=(1,2)):\n",
    "   \n",
    "\n",
    "    preds = []\n",
    "    refs = []\n",
    "\n",
    "    for ex in tqdm(ds_samsum, desc=\"SAMSum zero-shot\"):\n",
    "        convo = ex[\"dialogue\"]\n",
    "        prompt = (\n",
    "            f\"Summarize the following conversation in {summary_sentences[0]}-{summary_sentences[1]} sentences:\\n\\n\"\n",
    "            + convo\n",
    "            + \"\\n\\nSummary:\"\n",
    "        )\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=1024).to(DEVICE)\n",
    "        out = model.generate(**inputs, **GEN_KWARGS_SUM)\n",
    "        summary = tokenizer.decode(out[0], skip_special_tokens=True)\n",
    "        preds.append(summary.strip())\n",
    "        refs.append(ex[\"summary\"].strip())\n",
    "\n",
    "    return {\"preds\": preds, \"refs\": refs}\n",
    "\n",
    "\n",
    "ds_samsum = load_dataset(\"knkarthick/samsum\", split=\"test\")\n",
    "if max_examples:\n",
    "        ds_samsum = ds_samsum.select(range(min(len(ds_samsum), max_examples)))\n",
    "\n",
    "samsum_res = zero_shot_samsum_summarization(ds_samsum, summary_sentences=(1,2))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "190e62db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REF: Hannah needs Betty's number but Amanda doesn't have it. She needs to contact Larry.\n",
      "PRED: Larry called Hannah last time they were at the park together. Hannah doesn't know Larry well.\n",
      "------------------------------------------------------------\n",
      "REF: Eric and Rob are going to watch a stand-up on youtube.\n",
      "PRED: Eric and Rob are watching a stand-up on YouTube.\n",
      "------------------------------------------------------------\n",
      "REF: Lenny can't decide which trousers to buy. Bob advised Lenny on that topic. Lenny goes with Bob's advice to pick the trousers that are of best quality.\n",
      "PRED: Bob sends Lenny photos of his trousers. Lenny will buy the first pair or the third pair.\n",
      "------------------------------------------------------------\n",
      "REF: Emma will be home soon and she will let Will know.\n",
      "PRED: Will is going to pick Emma up. Emma will be home soon.\n",
      "------------------------------------------------------------\n",
      "REF: Jane is in Warsaw. Ollie and Jane has a party. Jane lost her calendar. They will get a lunch this week on Friday. Ollie accidentally called Jane and talked about whisky. Jane cancels lunch. They'll meet for a tea at 6 pm.\n",
      "PRED: Ollie is in Warsaw. Jane and Ollie have lunch this week. They have lunch on Friday. Ollie will bring some sun with her.\n",
      "------------------------------------------------------------\n",
      "REF: Hilary has the keys to the apartment. Benjamin wants to get them and go take a nap. Hilary is having lunch with some French people at La Cantina. Hilary is meeting them at the entrance to the conference hall at 2 pm. Benjamin and Elliot might join them. They're meeting for the drinks in the evening.\n",
      "PRED: Benjamin, Hilary and Daniel are meeting for drinks in the evening. Hilary and Daniel will go to La Cantina. Hilary will take the keys and take a nap.\n",
      "------------------------------------------------------------\n",
      "REF: Payton provides Max with websites selling clothes. Payton likes browsing and trying on the clothes but not necessarily buying them. Payton usually buys clothes and books as he loves reading.\n",
      "PRED: Max and Payton buy clothes from a lot of sites. Payton likes browsing, looking in the mirror, looking in the mirror and seeing how he looks, but doesn't always buying.\n",
      "------------------------------------------------------------\n",
      "REF: Rita and Tina are bored at work and have still 4 hours left.\n",
      "PRED: Rita is tired. Tina keeps on looking at the clock and there's still 4 hours of boredom.\n",
      "------------------------------------------------------------\n",
      "REF: Beatrice wants to buy Leo a scarf, but he doesn't like scarves. She cares about his health and will buy him a scarf no matter his opinion.\n",
      "PRED: Beatrice doesn't want to buy a scarf because she doesn't have a scarf.\n",
      "------------------------------------------------------------\n",
      "REF: Eric doesn't know if his parents let him go to Ivan's brother's wedding. Ivan will talk to them.\n",
      "PRED: Eric is coming to the wedding with his brother's. Ivan doesn't know if his parents will let him.\n",
      "------------------------------------------------------------\n",
      "REF: Wanda wants to throw a party. She asks Gina to borrow her father's car and go do groceries together. They set the date for Friday.\n",
      "PRED: Wanda and Gina will make a party on Friday.\n",
      "------------------------------------------------------------\n",
      "REF: Martin wrote a short review and won 2 cinema tickets on FB. Martin wants Aggie to go with him this week for the new film with Redford.\n",
      "PRED: Martin won two cinema tickets for the new film with Redford.\n",
      "------------------------------------------------------------\n",
      "REF: Charlee is attending Portuguese theater as a subject at university. He and other students are preparing a play by Mrożek translated into Portuguese.\n",
      "PRED: Charlee is in class. Curtis and Charlee are preparing a performance.\n",
      "------------------------------------------------------------\n",
      "REF: Ella rented a car, this makes things much faster for her and Tom.\n",
      "PRED: Tom and Ella are going by car or train.\n",
      "------------------------------------------------------------\n",
      "REF: Paul is going to share his Netflix account with Luke. In exchange Luke is going to contribute to the subscription. Paul will send Luke his bank details. Paul is on vacation with his girlfriend till tomorrow.\n",
      "PRED: Luke is looking for someone to join netflix family. Luke will send him the login and password on sunday. Luke will send him the bank account details so he can wire him the money every month. Luke is still on holidays with his girl. Luke has been there less than 8 days.\n",
      "------------------------------------------------------------\n",
      "REF: Greg and Betsy have a lot of work today, so they cannot pick up Johnny from the kindergarten. However, it's Greg's turn to do it. Greg will try to find a solution.\n",
      "PRED: Greg and Betsy are going to pick Johnny up. Greg will pick him up on Tuesday.\n",
      "------------------------------------------------------------\n",
      "REF: Ethan, Toby and Marshall are making fun of Scott.\n",
      "PRED: Scott and Toby are enjoying making fun of each other.\n",
      "------------------------------------------------------------\n",
      "REF: Igor has a lot of work on his notice period and he feels demotivated. John thinks he should do what he has to do nevertheless.\n",
      "PRED: Igor is demotivated to give much work to someone on their notice period.\n",
      "------------------------------------------------------------\n",
      "REF: Clara is rewatching Dear White People and strongly recommends it to Neela.\n",
      "PRED: Neela is watching Dear White People on Netflix.\n",
      "------------------------------------------------------------\n",
      "REF: Mike took his car into garage today. Ernest is relieved as someone had just crashed into a red Honda which looks like Mike's.\n",
      "PRED: Ernest parked his car on the street. Mike took it into garage today.\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Show a few summarization examples\n",
    "for i in range(20):\n",
    "    print(\"REF:\", samsum_res[\"refs\"][i])\n",
    "    print(\"PRED:\", samsum_res[\"preds\"][i])\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ae999a",
   "metadata": {},
   "source": [
    "## Summarization evaluation (ROUGE + BERTScore + bootstrap CI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "c1770688",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE results (medians / f1 where available):\n",
      "  rouge1: 0.4452\n",
      "  rouge2: 0.1987\n",
      "  rougeL: 0.3655\n",
      "  rougeLsum: 0.3654\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 40.00 MiB. GPU 0 has a total capacity of 3.68 GiB of which 41.00 MiB is free. Including non-PyTorch memory, this process has 3.62 GiB memory in use. Of the allocated memory 3.43 GiB is allocated by PyTorch, and 97.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[71]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# BERTScore\u001b[39;00m\n\u001b[32m     18\u001b[39m bertscore = evaluate.load(\u001b[33m\"\u001b[39m\u001b[33mbertscore\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m bs_res = \u001b[43mbertscore\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpreds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreferences\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrefs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlang\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43men\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel_type\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmicrosoft/deberta-xlarge-mnli\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# model_type optional\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mBERTScore (mean):\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  precision: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp.mean(bs_res[\u001b[33m'\u001b[39m\u001b[33mprecision\u001b[39m\u001b[33m'\u001b[39m])\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/AIandContent/GL-selflearning-sem2/flan-t5-finetuning/projects/lib/python3.12/site-packages/evaluate/module.py:467\u001b[39m, in \u001b[36mEvaluationModule.compute\u001b[39m\u001b[34m(self, predictions, references, **kwargs)\u001b[39m\n\u001b[32m    465\u001b[39m inputs = {input_name: \u001b[38;5;28mself\u001b[39m.data[input_name][:] \u001b[38;5;28;01mfor\u001b[39;00m input_name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._feature_names()}\n\u001b[32m    466\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m temp_seed(\u001b[38;5;28mself\u001b[39m.seed):\n\u001b[32m--> \u001b[39m\u001b[32m467\u001b[39m     output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcompute_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    469\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.buf_writer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    470\u001b[39m     \u001b[38;5;28mself\u001b[39m.buf_writer = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/huggingface/modules/evaluate_modules/metrics/evaluate-metric--bertscore/cf4907b18f8f741f202232c0f8009a3bd49ff98802c245abcb6ea51a37a8c05b/bertscore.py:203\u001b[39m, in \u001b[36mBERTScore._compute\u001b[39m\u001b[34m(self, predictions, references, lang, model_type, num_layers, verbose, idf, device, batch_size, nthreads, all_layers, rescale_with_baseline, baseline_path, use_fast_tokenizer)\u001b[39m\n\u001b[32m    188\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mcached_bertscorer\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.cached_bertscorer.hash != hashcode:\n\u001b[32m    189\u001b[39m         \u001b[38;5;28mself\u001b[39m.cached_bertscorer = scorer(\n\u001b[32m    190\u001b[39m             model_type=model_type,\n\u001b[32m    191\u001b[39m             num_layers=num_layers,\n\u001b[32m   (...)\u001b[39m\u001b[32m    200\u001b[39m             baseline_path=baseline_path,\n\u001b[32m    201\u001b[39m         )\n\u001b[32m--> \u001b[39m\u001b[32m203\u001b[39m (P, R, F) = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcached_bertscorer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscore\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    204\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcands\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpredictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    205\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrefs\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreferences\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    206\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    207\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    208\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    209\u001b[39m output_dict = {\n\u001b[32m    210\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mprecision\u001b[39m\u001b[33m\"\u001b[39m: P.tolist(),\n\u001b[32m    211\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mrecall\u001b[39m\u001b[33m\"\u001b[39m: R.tolist(),\n\u001b[32m    212\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mf1\u001b[39m\u001b[33m\"\u001b[39m: F.tolist(),\n\u001b[32m    213\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mhashcode\u001b[39m\u001b[33m\"\u001b[39m: hashcode,\n\u001b[32m    214\u001b[39m }\n\u001b[32m    215\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output_dict\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/AIandContent/GL-selflearning-sem2/flan-t5-finetuning/projects/lib/python3.12/site-packages/bert_score/scorer.py:220\u001b[39m, in \u001b[36mBERTScorer.score\u001b[39m\u001b[34m(self, cands, refs, verbose, batch_size, return_hash)\u001b[39m\n\u001b[32m    217\u001b[39m     idf_dict[\u001b[38;5;28mself\u001b[39m._tokenizer.sep_token_id] = \u001b[32m0\u001b[39m\n\u001b[32m    218\u001b[39m     idf_dict[\u001b[38;5;28mself\u001b[39m._tokenizer.cls_token_id] = \u001b[32m0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m220\u001b[39m all_preds = \u001b[43mbert_cos_score_idf\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    221\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrefs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcands\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    224\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_tokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[43m    \u001b[49m\u001b[43midf_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    226\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    227\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    228\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    229\u001b[39m \u001b[43m    \u001b[49m\u001b[43mall_layers\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mall_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    230\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m.cpu()\n\u001b[32m    232\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ref_group_boundaries \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    233\u001b[39m     max_preds = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/AIandContent/GL-selflearning-sem2/flan-t5-finetuning/projects/lib/python3.12/site-packages/bert_score/utils.py:616\u001b[39m, in \u001b[36mbert_cos_score_idf\u001b[39m\u001b[34m(model, refs, hyps, tokenizer, idf_dict, verbose, batch_size, device, all_layers)\u001b[39m\n\u001b[32m    614\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m batch_start \u001b[38;5;129;01min\u001b[39;00m iter_range:\n\u001b[32m    615\u001b[39m     sen_batch = sentences[batch_start : batch_start + batch_size]\n\u001b[32m--> \u001b[39m\u001b[32m616\u001b[39m     embs, masks, padded_idf = \u001b[43mget_bert_embedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    617\u001b[39m \u001b[43m        \u001b[49m\u001b[43msen_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midf_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mall_layers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mall_layers\u001b[49m\n\u001b[32m    618\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    619\u001b[39m     embs = embs.cpu()\n\u001b[32m    620\u001b[39m     masks = masks.cpu()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/AIandContent/GL-selflearning-sem2/flan-t5-finetuning/projects/lib/python3.12/site-packages/bert_score/utils.py:455\u001b[39m, in \u001b[36mget_bert_embedding\u001b[39m\u001b[34m(all_sens, model, tokenizer, idf_dict, batch_size, device, all_layers)\u001b[39m\n\u001b[32m    453\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(all_sens), batch_size):\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m         batch_embedding = \u001b[43mbert_encode\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m            \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpadded_sens\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    458\u001b[39m \u001b[43m            \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    459\u001b[39m \u001b[43m            \u001b[49m\u001b[43mall_layers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mall_layers\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    460\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    461\u001b[39m         embeddings.append(batch_embedding)\n\u001b[32m    462\u001b[39m         \u001b[38;5;28;01mdel\u001b[39;00m batch_embedding\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/AIandContent/GL-selflearning-sem2/flan-t5-finetuning/projects/lib/python3.12/site-packages/bert_score/utils.py:351\u001b[39m, in \u001b[36mbert_encode\u001b[39m\u001b[34m(model, x, attention_mask, all_layers)\u001b[39m\n\u001b[32m    349\u001b[39m model.eval()\n\u001b[32m    350\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m351\u001b[39m     out = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mall_layers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    352\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m all_layers:\n\u001b[32m    353\u001b[39m     emb = torch.stack(out[-\u001b[32m1\u001b[39m], dim=\u001b[32m2\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/AIandContent/GL-selflearning-sem2/flan-t5-finetuning/projects/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/AIandContent/GL-selflearning-sem2/flan-t5-finetuning/projects/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/AIandContent/GL-selflearning-sem2/flan-t5-finetuning/projects/lib/python3.12/site-packages/transformers/models/deberta/modeling_deberta.py:707\u001b[39m, in \u001b[36mDebertaModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, token_type_ids, position_ids, inputs_embeds, output_attentions, output_hidden_states, return_dict)\u001b[39m\n\u001b[32m    697\u001b[39m     token_type_ids = torch.zeros(input_shape, dtype=torch.long, device=device)\n\u001b[32m    699\u001b[39m embedding_output = \u001b[38;5;28mself\u001b[39m.embeddings(\n\u001b[32m    700\u001b[39m     input_ids=input_ids,\n\u001b[32m    701\u001b[39m     token_type_ids=token_type_ids,\n\u001b[32m   (...)\u001b[39m\u001b[32m    704\u001b[39m     inputs_embeds=inputs_embeds,\n\u001b[32m    705\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m707\u001b[39m encoder_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    708\u001b[39m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    709\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    710\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    711\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    712\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    713\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    714\u001b[39m encoded_layers = encoder_outputs[\u001b[32m1\u001b[39m]\n\u001b[32m    716\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.z_steps > \u001b[32m1\u001b[39m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/AIandContent/GL-selflearning-sem2/flan-t5-finetuning/projects/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/AIandContent/GL-selflearning-sem2/flan-t5-finetuning/projects/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/AIandContent/GL-selflearning-sem2/flan-t5-finetuning/projects/lib/python3.12/site-packages/transformers/models/deberta/modeling_deberta.py:583\u001b[39m, in \u001b[36mDebertaEncoder.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, output_hidden_states, output_attentions, query_states, relative_pos, return_dict)\u001b[39m\n\u001b[32m    581\u001b[39m rel_embeddings = \u001b[38;5;28mself\u001b[39m.get_rel_embedding()\n\u001b[32m    582\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, layer_module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.layer):\n\u001b[32m--> \u001b[39m\u001b[32m583\u001b[39m     hidden_states, att_m = \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    584\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnext_kv\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    585\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    586\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    587\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    588\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    589\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    590\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    592\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m output_hidden_states:\n\u001b[32m    593\u001b[39m         all_hidden_states = all_hidden_states + (hidden_states,)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/AIandContent/GL-selflearning-sem2/flan-t5-finetuning/projects/lib/python3.12/site-packages/transformers/modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/AIandContent/GL-selflearning-sem2/flan-t5-finetuning/projects/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/AIandContent/GL-selflearning-sem2/flan-t5-finetuning/projects/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/AIandContent/GL-selflearning-sem2/flan-t5-finetuning/projects/lib/python3.12/site-packages/transformers/models/deberta/modeling_deberta.py:511\u001b[39m, in \u001b[36mDebertaLayer.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, query_states, relative_pos, rel_embeddings, output_attentions)\u001b[39m\n\u001b[32m    502\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    503\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    504\u001b[39m     hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    509\u001b[39m     output_attentions: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    510\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[torch.Tensor, Optional[torch.Tensor]]:\n\u001b[32m--> \u001b[39m\u001b[32m511\u001b[39m     attention_output, att_matrix = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    512\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    513\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    514\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m=\u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    515\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    516\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    517\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    518\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    519\u001b[39m     intermediate_output = \u001b[38;5;28mself\u001b[39m.intermediate(attention_output)\n\u001b[32m    520\u001b[39m     layer_output = \u001b[38;5;28mself\u001b[39m.output(intermediate_output, attention_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/AIandContent/GL-selflearning-sem2/flan-t5-finetuning/projects/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/AIandContent/GL-selflearning-sem2/flan-t5-finetuning/projects/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/AIandContent/GL-selflearning-sem2/flan-t5-finetuning/projects/lib/python3.12/site-packages/transformers/models/deberta/modeling_deberta.py:446\u001b[39m, in \u001b[36mDebertaAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[39m\n\u001b[32m    437\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m    438\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m    439\u001b[39m     hidden_states,\n\u001b[32m   (...)\u001b[39m\u001b[32m    444\u001b[39m     rel_embeddings=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    445\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[torch.Tensor, Optional[torch.Tensor]]:\n\u001b[32m--> \u001b[39m\u001b[32m446\u001b[39m     self_output, att_matrix = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    448\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    449\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    452\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    454\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m query_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    455\u001b[39m         query_states = hidden_states\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/AIandContent/GL-selflearning-sem2/flan-t5-finetuning/projects/lib/python3.12/site-packages/torch/nn/modules/module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/AIandContent/GL-selflearning-sem2/flan-t5-finetuning/projects/lib/python3.12/site-packages/torch/nn/modules/module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/AIandContent/GL-selflearning-sem2/flan-t5-finetuning/projects/lib/python3.12/site-packages/transformers/models/deberta/modeling_deberta.py:266\u001b[39m, in \u001b[36mDisentangledSelfAttention.forward\u001b[39m\u001b[34m(self, hidden_states, attention_mask, output_attentions, query_states, relative_pos, rel_embeddings)\u001b[39m\n\u001b[32m    264\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.relative_attention \u001b[38;5;129;01mand\u001b[39;00m rel_embeddings \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m relative_pos \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    265\u001b[39m     rel_embeddings = \u001b[38;5;28mself\u001b[39m.pos_dropout(rel_embeddings)\n\u001b[32m--> \u001b[39m\u001b[32m266\u001b[39m     rel_att = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdisentangled_att_bias\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrelative_pos\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrel_embeddings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_factor\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m rel_att \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    269\u001b[39m     attention_scores = attention_scores + rel_att\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/learn/AIandContent/GL-selflearning-sem2/flan-t5-finetuning/projects/lib/python3.12/site-packages/transformers/models/deberta/modeling_deberta.py:322\u001b[39m, in \u001b[36mDisentangledSelfAttention.disentangled_att_bias\u001b[39m\u001b[34m(self, query_layer, key_layer, relative_pos, rel_embeddings, scale_factor)\u001b[39m\n\u001b[32m    320\u001b[39m pos_key_layer = \u001b[38;5;28mself\u001b[39m.pos_proj(rel_embeddings)\n\u001b[32m    321\u001b[39m pos_key_layer = \u001b[38;5;28mself\u001b[39m.transpose_for_scores(pos_key_layer)\n\u001b[32m--> \u001b[39m\u001b[32m322\u001b[39m c2p_att = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_layer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpos_key_layer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    323\u001b[39m c2p_pos = torch.clamp(relative_pos + att_span, \u001b[32m0\u001b[39m, att_span * \u001b[32m2\u001b[39m - \u001b[32m1\u001b[39m)\n\u001b[32m    324\u001b[39m c2p_att = torch.gather(c2p_att, dim=-\u001b[32m1\u001b[39m, index=c2p_dynamic_expand(c2p_pos, query_layer, relative_pos))\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 40.00 MiB. GPU 0 has a total capacity of 3.68 GiB of which 41.00 MiB is free. Including non-PyTorch memory, this process has 3.62 GiB memory in use. Of the allocated memory 3.43 GiB is allocated by PyTorch, and 97.57 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "# preds and refs from your notebook: samsum_res[\"preds\"], samsum_res[\"refs\"]\n",
    "preds = samsum_res[\"preds\"]\n",
    "refs  = samsum_res[\"refs\"]\n",
    "\n",
    "# ROUGE\n",
    "rouge = evaluate.load(\"rouge\")\n",
    "rouge_res = rouge.compute(predictions=preds, references=refs)\n",
    "print(\"ROUGE results (medians / f1 where available):\")\n",
    "for k, v in rouge_res.items():\n",
    "    # evaluate returns e.g. {'rouge1': 0.4, 'rouge2': 0.2, 'rougeL': 0.37}\n",
    "    print(f\"  {k}: {v:.4f}\")\n",
    "\n",
    "# BERTScore\n",
    "bertscore = evaluate.load(\"bertscore\")\n",
    "bs_res = bertscore.compute(predictions=preds, references=refs, lang=\"en\", model_type=\"microsoft/deberta-xlarge-mnli\")  # model_type optional\n",
    "print(\"\\nBERTScore (mean):\")\n",
    "print(f\"  precision: {np.mean(bs_res['precision']):.4f}\")\n",
    "print(f\"  recall:    {np.mean(bs_res['recall']):.4f}\")\n",
    "print(f\"  f1:        {np.mean(bs_res['f1']):.4f}\")\n",
    "\n",
    "# Bootstrapped CI for ROUGE-1 F1\n",
    "def rouge1_f1(preds_subset, refs_subset):\n",
    "    r = evaluate.load(\"rouge\")\n",
    "    res = r.compute(predictions=preds_subset, references=refs_subset)\n",
    "    return res[\"rouge1\"]\n",
    "\n",
    "def bootstrap_rouge(preds, refs, n_boot=1000, alpha=0.05, seed=42):\n",
    "    rng = random.Random(seed)\n",
    "    n = len(preds)\n",
    "    stats = []\n",
    "    for _ in range(n_boot):\n",
    "        idxs = [rng.randrange(n) for _ in range(n)]\n",
    "        p_sample = [preds[i] for i in idxs]\n",
    "        r_sample = [refs[i] for i in idxs]\n",
    "        stats.append(rouge1_f1(p_sample, r_sample))\n",
    "    stats = np.array(stats)\n",
    "    lo = np.percentile(stats, 100 * (alpha/2))\n",
    "    hi = np.percentile(stats, 100 * (1-alpha/2))\n",
    "    return lo, hi\n",
    "\n",
    "r1_lo, r1_hi = bootstrap_rouge(preds, refs, n_boot=500)  # reduce n_boot for speed on Colab\n",
    "print(f\"\\nROUGE-1 F1 95% CI (bootstrap, n_boot=500): [{r1_lo:.4f}, {r1_hi:.4f}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "1113dc94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved readable .txt and JSON files in ./outputs/\n"
     ]
    }
   ],
   "source": [
    "# Save predictions to disk for later analysis\n",
    "import json\n",
    "\n",
    "\n",
    "\n",
    "def save_outputs(df, dir, file_name):\n",
    "    os.makedirs(dir, exist_ok=True)\n",
    "    with open(os.path.join(dir, file_name), \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(df, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "def format_sst2_readable(res):\n",
    "    lines = [\n",
    "        f\"{i}\\tPRED={p}\\tTRUE={t}\\tSENT={s}\"\n",
    "        for i, (s, p, t) in enumerate(zip(res[\"sentence\"], res[\"preds\"], res[\"trues\"]))\n",
    "    ]\n",
    "    return lines\n",
    "\n",
    "def format_samsum_readable(res):\n",
    "    lines = [\n",
    "        f\"{i}\\nREF: {r}\\nPRED: {p}\\n\" + \"-\"*60\n",
    "        for i, (r, p) in enumerate(zip(res[\"refs\"], res[\"preds\"]))\n",
    "    ]\n",
    "    return lines\n",
    "\n",
    "def write_text(lines, dir, file_name):\n",
    "    os.makedirs(dir, exist_ok=True)\n",
    "    with open(os.path.join(dir, file_name), \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(lines))\n",
    "\n",
    "# Create readable text versions\n",
    "sst2_readable = format_sst2_readable(sst2_res)\n",
    "samsum_readable = format_samsum_readable(samsum_res)\n",
    "\n",
    "# Write text files\n",
    "write_text(sst2_readable, \"outputs\", \"sst2_preds-zeroshot.txt\")\n",
    "write_text(samsum_readable, \"outputs\", \"samsum_preds-zeroshot.txt\")\n",
    "\n",
    "# Write JSON files\n",
    "save_outputs(sst2_res, \"outputs\", \"sst2_preds-zeroshot.json\")\n",
    "save_outputs(samsum_res, \"outputs\", \"samsum_preds-zeroshot.json\")\n",
    "\n",
    "print(\"Saved readable .txt and JSON files in ./outputs/\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "projects",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
